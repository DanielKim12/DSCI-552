{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>KIM_GUKIL_FINAL_PROJECT</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: GUK IL KIM \n",
    "<br>\n",
    "Github Username: DanielKim12\n",
    "<br>\n",
    "USC ID: 3020867072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import ResNet50, EfficientNetB0, VGG16\n",
    "from keras import layers, models, Model\n",
    "# from keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "import shutil\n",
    "import splitfolders\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5}\n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "train_dir = \"../data/seg_train\"\n",
    "test_dir = \"../data/seg_test\"\n",
    "\n",
    "# Define target image size (e.g., 128x128)\n",
    "TARGET_SIZE = (128, 128)\n",
    "# Create class-to-index mapping for one-hot encoding\n",
    "classes = sorted(os.listdir(train_dir))  # Sort to keep consistent order\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n",
    "print(\"Classes:\", class_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 files belonging to 6 classes.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Found 9820 files belonging to 6 classes.\n",
      "Found 4214 files belonging to 6 classes.\n",
      "Training data: <_MapDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 6), dtype=tf.float32, name=None))>\n",
      "Validation data: <_MapDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 6), dtype=tf.float32, name=None))>\n",
      "Test data: <_MapDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 6), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Prepare function for preprocessing\n",
    "def prepare_data(path, image_dim=(128, 128), batch_size=32):\n",
    "    # Preprocess and load data from a directory\n",
    "    data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        path,\n",
    "        image_size=image_dim,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    # Map one-hot encoding to labels\n",
    "    return data.map(lambda x, y: (x, tf.one_hot(y, depth=len(class_to_index))))\n",
    "\n",
    "# Preprocess train and test datasets directly\n",
    "df_train = prepare_data(train_dir)  # Prepare training data\n",
    "df_test = prepare_data(test_dir)  # Prepare test data\n",
    "\n",
    "# Optionally split training data into train/validation subsets\n",
    "val_split_path = \"./temp\"  # Temporary directory for splitting\n",
    "shutil.rmtree(val_split_path, ignore_errors=True)  # Clean up previous splits if any\n",
    "\n",
    "# Split seg_train into train (70%) and validation (30%) using a temporary directory\n",
    "splitfolders.ratio(train_dir, val_split_path, seed=42, ratio=(0.7, 0.3))\n",
    "\n",
    "# Prepare train and validation datasets\n",
    "df_train = prepare_data(val_split_path + \"/train\")\n",
    "df_val = prepare_data(val_split_path + \"/val\")\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"Training data: {df_train}\")\n",
    "print(f\"Validation data: {df_val}\")\n",
    "print(f\"Test data: {df_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = models.Sequential(\n",
    "    [\n",
    "        layers.RandomZoom(height_factor=0.1),\n",
    "        layers.RandomRotation(factor=0.1),\n",
    "        layers.RandomFlip(),\n",
    "        layers.RandomContrast(factor=0.1),\n",
    "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "    ],\n",
    "    name=\"augment\",\n",
    ")\n",
    "\n",
    "def get_model(model_name, inputs):\n",
    "    if model_name == \"ResNet50\":\n",
    "        return ResNet50(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "    if model_name == \"EfficientNetB0\":\n",
    "        return EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "    if model_name == \"VGG16\":\n",
    "        return VGG16(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "    raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "\n",
    "def build_model(num_classes, model_name):\n",
    "    inputs = layers.Input(shape=(128, 128, 3))  # Define input layer\n",
    "    data = augment(inputs)  # Apply data augmentation\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    base_model = get_model(model_name, data)\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "\n",
    "    # Add custom layers\n",
    "    data = layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model.output)\n",
    "    data = layers.Activation(\"relu\")(data)\n",
    "    data = layers.BatchNormalization()(data)\n",
    "    data = layers.Dropout(0.2)(data)  # Dropout rate of 20%\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(data)  # Correct number of classes\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def get_callbacks(model_name):\n",
    "    save_dir = \"./pre-trained\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    return [\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(save_dir, f\"{model_name}.weights.h5\"),\n",
    "            save_weights_only=True,\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=10,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "def train_model(model_name, df_train, df_val, num_classes):\n",
    "    # Build the model\n",
    "    model = build_model(num_classes, model_name)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        df_train,\n",
    "        validation_data=df_val,\n",
    "        epochs=50,  # Adjust based on your computational resources\n",
    "        callbacks=get_callbacks(model_name),\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "resnet_model, resnet_history = train_model(\"ResNet50\", df_train, df_val, num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images Shape: (14034, 128, 128, 3)\n",
      "Training Labels Shape: (14034, 6)\n",
      "Test Images Shape: (3000, 128, 128, 3)\n",
      "Test Labels Shape: (3000, 6)\n",
      "Class-to-Index Mapping: {'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define augmentation functions\n",
    "def random_rotation(image, angle_range=20):\n",
    "    angle = np.random.uniform(-angle_range, angle_range)\n",
    "    h, w = image.shape[:2]\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "    return cv2.warpAffine(image, rotation_matrix, (w, h))\n",
    "\n",
    "def random_translation(image, max_shift=0.2):\n",
    "    h, w = image.shape[:2]\n",
    "    tx = np.random.uniform(-max_shift * w, max_shift * w)\n",
    "    ty = np.random.uniform(-max_shift * h, max_shift * h)\n",
    "    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "    return cv2.warpAffine(image, translation_matrix, (w, h))\n",
    "\n",
    "def random_zoom(image, zoom_range=0.2):\n",
    "    h, w = image.shape[:2]\n",
    "    scale = np.random.uniform(1 - zoom_range, 1 + zoom_range)\n",
    "    zoom_matrix = cv2.getRotationMatrix2D((w // 2, h // 2), 0, scale)\n",
    "    return cv2.warpAffine(image, zoom_matrix, (w, h))\n",
    "\n",
    "def random_flip(image):\n",
    "    if np.random.choice([True, False]):\n",
    "        return cv2.flip(image, 1)  # Horizontal flip\n",
    "    return image\n",
    "\n",
    "def random_contrast(image, contrast_range=(0.8, 1.2)):\n",
    "    factor = np.random.uniform(*contrast_range)\n",
    "    return cv2.convertScaleAbs(image, alpha=factor, beta=0)\n",
    "\n",
    "def augment_image(image):\n",
    "    image = random_rotation(image)\n",
    "    image = random_translation(image)\n",
    "    image = random_zoom(image)\n",
    "    image = random_flip(image)\n",
    "    image = random_contrast(image)\n",
    "    return image\n",
    "\n",
    "# Step 2: Load and preprocess images\n",
    "def load_and_augment_images(directory, target_size=(128, 128)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = sorted(os.listdir(directory))  # Class names from folder structure\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(classes)}  # Map classes to indices\n",
    "\n",
    "    for class_name, class_index in class_to_index.items():\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_name)\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is not None:\n",
    "                    image = cv2.resize(image, target_size)  # Resize image\n",
    "                    image = augment_image(image)  # Apply augmentations\n",
    "                    images.append(image)\n",
    "                    labels.append(class_index)\n",
    "\n",
    "    images = np.array(images, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    labels = np.array(labels)\n",
    "    return images, labels, class_to_index\n",
    "\n",
    "# Step 3: Apply one-hot encoding\n",
    "def preprocess_dataset(directory, target_size=(128, 128), num_classes=None):\n",
    "    images, labels, class_to_index = load_and_augment_images(directory, target_size)\n",
    "    if num_classes is None:\n",
    "        num_classes = len(class_to_index)  # Infer number of classes\n",
    "    labels = to_categorical(labels, num_classes=num_classes)  # One-hot encode labels\n",
    "    return images, labels, class_to_index\n",
    "\n",
    "# Step 4: Process training and test datasets\n",
    "train_dir = \"../data/seg_train\"\n",
    "test_dir = \"../data/seg_test\"\n",
    "\n",
    "train_images, train_labels, class_to_index = preprocess_dataset(train_dir)\n",
    "test_images, test_labels, _ = preprocess_dataset(test_dir, num_classes=len(class_to_index))\n",
    "\n",
    "print(\"Training Images Shape:\", train_images.shape)\n",
    "print(\"Training Labels Shape:\", train_labels.shape)\n",
    "print(\"Test Images Shape:\", test_images.shape)\n",
    "print(\"Test Labels Shape:\", test_labels.shape)\n",
    "print(\"Class-to-Index Mapping:\", class_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,851,270</span> (90.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,851,270\u001b[0m (90.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">263,302</span> (1.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m263,302\u001b[0m (1.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,968</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,968\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Load pre-trained model\n",
    "def load_pretrained_model(model_name, input_shape):\n",
    "    if model_name == \"ResNet50\":\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    elif model_name == \"ResNet101\":\n",
    "        base_model = tf.keras.applications.ResNet101(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    elif model_name == \"EfficientNetB0\":\n",
    "        base_model = tf.keras.applications.EfficientNetB0(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    elif model_name == \"VGG16\":\n",
    "        base_model = tf.keras.applications.VGG16(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    # Freeze all layers in the pre-trained model\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Step 2: Build transfer learning model\n",
    "def build_transfer_learning_model(model_name, num_classes, input_shape=(128, 128, 3)):\n",
    "    # Load the pre-trained base model\n",
    "    base_model = load_pretrained_model(model_name, input_shape)\n",
    "    \n",
    "    # Add custom layers\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)  # Pass inputs through the base model\n",
    "    x = layers.GlobalAveragePooling2D(name=\"global_avg_pool\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Dropout rate of 20%\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)  # Output layer\n",
    "    \n",
    "    # Build the model\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Step 3: Set parameters and build the model\n",
    "num_classes = len(class_to_index)  # From your dataset\n",
    "model_name = \"ResNet50\"  # Change to \"ResNet101\", \"EfficientNetB0\", or \"VGG16\" as needed\n",
    "input_shape = (128, 128, 3)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_transfer_learning_model(model_name, num_classes, input_shape)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define callbacks\n",
    "def get_callbacks(model_name):\n",
    "    # Ensure the directory exists to save weights\n",
    "    save_dir = \"./pretrained_weights\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    return [\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(save_dir, f\"{model_name}_best.weights.h5\"),  # Add `.weights.h5` extension\n",
    "            monitor=\"val_accuracy\",  # Monitor validation accuracy\n",
    "            save_best_only=True,    # Save only the best weights\n",
    "            save_weights_only=True, # Save only weights, not the full model\n",
    "            mode=\"max\"              # Maximize validation accuracy\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",     # Monitor validation loss\n",
    "            patience=10,            # Stop training if no improvement for 10 epochs\n",
    "            restore_best_weights=True  # Restore the best weights at the end\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Step 2: Train the model\n",
    "def train_model(model, train_dataset, val_dataset, model_name, epochs=50, batch_size=5):\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Get callbacks\n",
    "    callbacks = get_callbacks(model_name)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_images and train_labels are prepared\n",
    "train_images, train_labels, _ = preprocess_dataset(\"../data/seg_train\")\n",
    "val_images, val_labels, _ = preprocess_dataset(\"../data/seg_test\", num_classes=num_classes)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x=train_images,\n",
    "    y=train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=50,\n",
    "    batch_size=5,\n",
    "    callbacks=get_callbacks(model_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 18:10:29.103155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images Shape: (14034, 128, 128, 3)\n",
      "Training Labels Shape: (14034, 6)\n",
      "Validation Images Shape: (3000, 128, 128, 3)\n",
      "Validation Labels Shape: (3000, 6)\n",
      "Class-to-Index Mapping: {'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,851,270</span> (90.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,851,270\u001b[0m (90.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">263,302</span> (1.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m263,302\u001b[0m (1.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,968</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,968\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras imports\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications import ResNet50, EfficientNetB0, VGG16\n",
    "from keras import layers, models, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##############################################################################\n",
    "#                          DATA AUGMENTATION FUNCTIONS                       #\n",
    "##############################################################################\n",
    "\n",
    "def random_rotation(image, angle_range=20):\n",
    "    angle = np.random.uniform(-angle_range, angle_range)\n",
    "    h, w = image.shape[:2]\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "    return cv2.warpAffine(image, rotation_matrix, (w, h))\n",
    "\n",
    "def random_translation(image, max_shift=0.2):\n",
    "    h, w = image.shape[:2]\n",
    "    tx = np.random.uniform(-max_shift * w, max_shift * w)\n",
    "    ty = np.random.uniform(-max_shift * h, max_shift * h)\n",
    "    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "    return cv2.warpAffine(image, translation_matrix, (w, h))\n",
    "\n",
    "def random_zoom(image, zoom_range=0.2):\n",
    "    h, w = image.shape[:2]\n",
    "    scale = np.random.uniform(1 - zoom_range, 1 + zoom_range)\n",
    "    zoom_matrix = cv2.getRotationMatrix2D((w // 2, h // 2), 0, scale)\n",
    "    return cv2.warpAffine(image, zoom_matrix, (w, h))\n",
    "\n",
    "def random_flip(image):\n",
    "    if np.random.choice([True, False]):\n",
    "        return cv2.flip(image, 1)  # Horizontal flip\n",
    "    return image\n",
    "\n",
    "def random_contrast(image, contrast_range=(0.8, 1.2)):\n",
    "    factor = np.random.uniform(*contrast_range)\n",
    "    return cv2.convertScaleAbs(image, alpha=factor, beta=0)\n",
    "\n",
    "def augment_image(image):\n",
    "    image = random_rotation(image)\n",
    "    image = random_translation(image)\n",
    "    image = random_zoom(image)\n",
    "    image = random_flip(image)\n",
    "    image = random_contrast(image)\n",
    "    return image\n",
    "\n",
    "##############################################################################\n",
    "#                    LOADING & PREPROCESSING DATASETS                        #\n",
    "##############################################################################\n",
    "\n",
    "def load_images_from_directory(directory, target_size=(64, 64), \n",
    "                               augment=False, class_to_index=None):\n",
    "    \"\"\"\n",
    "    Loads and (optionally) augments images from a folder structure.\n",
    "    directory: path containing subfolders for each class.\n",
    "    augment: set True only for training data; usually False for test data.\n",
    "    class_to_index: optional dict that forces a specific label order.\n",
    "                    If None, we create a new one from scratch.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # If class_to_index not provided, build it from the subfolder names\n",
    "    if class_to_index is None:\n",
    "        classes = sorted(os.listdir(directory))\n",
    "        class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n",
    "    \n",
    "    # Now loop over the classes and load images\n",
    "    for class_name, class_index in class_to_index.items():\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is not None:\n",
    "                    # Resize to target shape\n",
    "                    image = cv2.resize(image, target_size)\n",
    "                    # Optionally apply augmentation\n",
    "                    if augment:\n",
    "                        image = augment_image(image)\n",
    "                    images.append(image)\n",
    "                    labels.append(class_index)\n",
    "\n",
    "    images = np.array(images, dtype=np.float32) / 255.0  # Normalize [0,1]\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    return images, labels, class_to_index\n",
    "\n",
    "def preprocess_dataset(directory, target_size=(64, 64), \n",
    "                       augment=False, class_to_index=None, num_classes=None):\n",
    "    \"\"\"\n",
    "    Loads images from directory, optionally augments them, normalizes, \n",
    "    and applies one-hot encoding.\n",
    "    \"\"\"\n",
    "    images, labels, class_to_index = load_images_from_directory(\n",
    "        directory=directory,\n",
    "        target_size=target_size,\n",
    "        augment=augment,\n",
    "        class_to_index=class_to_index\n",
    "    )\n",
    "    \n",
    "    # If user didn't provide num_classes, infer from dict\n",
    "    if num_classes is None:\n",
    "        num_classes = len(class_to_index)\n",
    "    \n",
    "    labels = to_categorical(labels, num_classes=num_classes)  # One-hot\n",
    "    return images, labels, class_to_index\n",
    "\n",
    "##############################################################################\n",
    "#                   BUILDING THE TRANSFER LEARNING MODEL                     #\n",
    "##############################################################################\n",
    "\n",
    "def load_pretrained_model(model_name, input_shape):\n",
    "    if model_name == \"ResNet50\":\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    elif model_name == \"ResNet101\":\n",
    "        base_model = tf.keras.applications.ResNet101(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    elif model_name == \"EfficientNetB0\":\n",
    "        base_model = tf.keras.applications.EfficientNetB0(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    elif model_name == \"VGG16\":\n",
    "        base_model = tf.keras.applications.VGG16(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    # Freeze all layers\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "def build_transfer_learning_model(model_name, num_classes, input_shape=(128, 128, 3)):\n",
    "    base_model = load_pretrained_model(model_name, input_shape)\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Pass through the pretrained base\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D(name=\"global_avg_pool\")(x)\n",
    "    # Add your custom layers\n",
    "    x = layers.Dense(128, activation=\"relu\",\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)  # 20% dropout\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "##############################################################################\n",
    "#                           CALLBACKS FOR TRAINING                           #\n",
    "##############################################################################\n",
    "\n",
    "def get_callbacks(model_name):\n",
    "    # Directory to save weights\n",
    "    save_dir = \"./pretrained_weights\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    return [\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(save_dir, f\"{model_name}_best.weights.h5\"),\n",
    "            monitor=\"val_accuracy\",  \n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"max\"\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "##############################################################################\n",
    "#                               MAIN EXECUTION                               #\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to your image folders\n",
    "    train_dir = \"../data/seg_train\"\n",
    "    test_dir  = \"../data/seg_test\"\n",
    "\n",
    "    # 1. Load & preprocess training data (with augmentation)\n",
    "        train_images, train_labels, _ = preprocess_dataset(\n",
    "        directory=train_dir,\n",
    "        target_size=(128, 128),\n",
    "        augment=False  # Turn off augmentation to see if the kernel stops dying\n",
    "    )\n",
    "    num_classes = len(class_to_index)\n",
    "\n",
    "    # 2. Load & preprocess test data (NO augmentation)\n",
    "    val_images, val_labels, _ = preprocess_dataset(\n",
    "        directory=test_dir,\n",
    "        target_size=(128, 128),\n",
    "        augment=False,  # Usually we don't augment test/validation\n",
    "        class_to_index=class_to_index,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    print(\"Training Images Shape:\", train_images.shape)\n",
    "    print(\"Training Labels Shape:\", train_labels.shape)\n",
    "    print(\"Validation Images Shape:\", val_images.shape)\n",
    "    print(\"Validation Labels Shape:\", val_labels.shape)\n",
    "    print(\"Class-to-Index Mapping:\", class_to_index)\n",
    "\n",
    "    # 3. Build the transfer learning model\n",
    "    model_name = \"ResNet50\"  # Could be \"ResNet101\", \"EfficientNetB0\", \"VGG16\", etc.\n",
    "    input_shape = (128, 128, 3)\n",
    "    model = build_transfer_learning_model(model_name, num_classes, input_shape)\n",
    "\n",
    "    # 4. Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # 5. Train the model\n",
    "    callbacks = get_callbacks(model_name)\n",
    "    history = model.fit(\n",
    "        x=train_images,\n",
    "        y=train_labels,\n",
    "        validation_data=(val_images, val_labels),\n",
    "        epochs=50,\n",
    "        batch_size=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # 6. Plot training vs. validation metrics\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Val Accuracy')\n",
    "    plt.title('Accuracy vs. Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "    plt.title('Loss vs. Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5}\n",
      "All images have been resized in place.\n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "train_dir = \"../data/seg_train\"\n",
    "test_dir = \"../data/seg_test\"\n",
    "\n",
    "# Define target image size (e.g., 128x128)\n",
    "TARGET_SIZE = (128, 128)\n",
    "# Create class-to-index mapping for one-hot encoding\n",
    "classes = sorted(os.listdir(train_dir))  # Sort to keep consistent order\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n",
    "print(\"Classes:\", class_to_index)\n",
    "num_classes = len(classes)\n",
    "\n",
    "def resize_images_in_place(directory, target_size):\n",
    "    for category in os.listdir(directory):  # Loop through each category folder\n",
    "        category_path = os.path.join(directory, category)\n",
    "        if not os.path.isdir(category_path):  # Skip non-folder files\n",
    "            continue\n",
    "\n",
    "        for image_name in os.listdir(category_path):  # Loop through each image\n",
    "            image_path = os.path.join(category_path, image_name)\n",
    "\n",
    "            # Read and process the image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Error loading image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Resize the image to the target size\n",
    "            resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Overwrite the original image with the resized version\n",
    "            cv2.imwrite(image_path, resized_image)\n",
    "\n",
    "# Resize train and test images\n",
    "resize_images_in_place(train_dir, TARGET_SIZE)\n",
    "resize_images_in_place(test_dir, TARGET_SIZE)\n",
    "\n",
    "print(\"All images have been resized in place.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 files belonging to 6 classes.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Train and test datasets are ready.\n"
     ]
    }
   ],
   "source": [
    "# Function to load images and apply one-hot encoding\n",
    "def load_data_with_one_hot(directory, target_size, batch_size=32):\n",
    "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory,\n",
    "        image_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"int\"  # Get labels as integers\n",
    "    )\n",
    "\n",
    "    # Apply one-hot encoding to labels\n",
    "    dataset = dataset.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes)))\n",
    "    return dataset\n",
    "\n",
    "# Load train and test datasets with one-hot encoded labels\n",
    "train_dataset = load_data_with_one_hot(train_dir, TARGET_SIZE)\n",
    "test_dataset = load_data_with_one_hot(test_dir, TARGET_SIZE)\n",
    "\n",
    "print(\"Train and test datasets are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a pre-trained model\n",
    "def load_pretrained_model(model_name, input_shape):\n",
    "    if model_name == \"ResNet50\":\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    elif model_name == \"ResNet101\":\n",
    "        base_model = tf.keras.applications.ResNet101(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    elif model_name == \"EfficientNetB0\":\n",
    "        base_model = tf.keras.applications.EfficientNetB0(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    elif model_name == \"VGG16\":\n",
    "        base_model = tf.keras.applications.VGG16(\n",
    "            include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    # Freeze all layers in the pre-trained model\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "def build_transfer_learning_model(model_name, num_classes, input_shape=(128, 128, 3)):\n",
    "    # Load the pre-trained base model\n",
    "    base_model = load_pretrained_model(model_name, input_shape)\n",
    "    \n",
    "    # Add custom layers\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)  # Pass inputs through the base model\n",
    "    x = layers.GlobalAveragePooling2D(name=\"global_avg_pool\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)  # Dropout rate of 20%\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)  # Output layer\n",
    "    \n",
    "    # Build the model\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,851,270</span> (90.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,851,270\u001b[0m (90.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">263,302</span> (1.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m263,302\u001b[0m (1.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,968</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,968\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"ResNet50\"  # Change to ResNet101, EfficientNetB0, or VGG16 as needed\n",
    "model = build_transfer_learning_model(model_name, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator with augmentations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,         # Randomly rotate images\n",
    "    width_shift_range=0.2,     # Randomly translate images horizontally\n",
    "    height_shift_range=0.2,    # Randomly translate images vertically\n",
    "    shear_range=0.2,           # Shear transformations\n",
    "    zoom_range=0.2,            # Randomly zoom images\n",
    "    horizontal_flip=True,      # Randomly flip images horizontally\n",
    "    validation_split=0.2       # Reserve 20% for validation\n",
    ")\n",
    "\n",
    "# Prepare training and validation datasets\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    \"../data/seg_train\",  # Training directory\n",
    "    target_size=(128, 128),\n",
    "    batch_size=5,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_dataset = train_datagen.flow_from_directory(\n",
    "    \"../data/seg_train\",  # Validation directory\n",
    "    target_size=(128, 128),\n",
    "    batch_size=5,\n",
    "    subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=./best_model_weights.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define callbacks\u001b[39;00m\n\u001b[1;32m     10\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./best_model_weights.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     16\u001b[0m     EarlyStopping(\n\u001b[1;32m     17\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m         patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# Stop if no improvement for 10 epochs\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m ]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, train_dataset, val_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.9/site-packages/keras/src/callbacks/model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[0;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=./best_model_weights.h5"
     ]
    }
   ],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=\"./best_model_weights.h5\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,  # Stop if no improvement for 10 epochs\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_dataset, val_dataset, epochs=50):\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
